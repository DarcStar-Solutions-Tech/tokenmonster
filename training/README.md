## Training tokenmonster

First prepare your dataset. The "extract_text_from_jsonl_parquet.py" file can help with this, as many of these datasets are in either jsonl or parquet. To use for generating a tokenizer you need raw text.
It's important that the dataset represents how you want to tokenize. You need to carefully consider what is in it. For example, if it's largely formal language, then the tokenizer will prefer formal language and might decide informal language is not worth giving tokens to. If you want a balance between English and French, then you'll need to have 50% English and 50% French in the training data. You see?

Then you'll need to compile "getalltokens.go" and "trainvocab.go", preferably with Go version of at least 1.20:
```
go mod init tokenmonster
go mod tidy
go build getalltokens.go
go build trainvocab.go
go build exporttokens.go
```

First run ./getalltokens on your training data. The default settings require around 100GB of RAM. If you don't have that you'll need to reduce `-chunk-size` to something much lower, like 10,000,000 bytes or even 1,000,000. However, you then need to consider the effect of -min-occur-chunk, perhaps reduce it to 2. You can probably increase `-min-occur` to something much higher without losing any quality, I kept it low just in case.
```
Usage of ./getalltokens:
  -chunk-size int
        the number of bytes processed at a time, you need around 1000x this much RAM, so 10GB of RAM for 10MB chunk-size (default 100000000)
  -dataset string
        filename of the dataset plain-text (required)
  -max-token-length int
        the maximum length of a token (default 30)
  -min-occur int
        tokens will be trimmed if they occur less frequently than this in the dataset (default 50)
  -min-occur-chunk int
        tokens will be trimmed if they occur less frequently than this per chunk (default 3)
  -output string
        output filename for the dictionary(required)
```

Then run ./trainvocab. It'll use `-dir` to store various states, which it's working, which you can also use to resume it from that point if for any reason you stop and start it again. Don't forget to set `-workers` to the number of CPU threads you have minus 1. You can speed it up by reducing `-midway-target` to 200,000 and `-overlap` to 1, but the final vocabulary might be less optimal.
```
Usage of ./trainvocab:
  -dataset string
        filename of the dataset plain-text (required)
  -dictionary string
        filename of the dictionary generated by makedictionary or any of the saved output files from this app (required)
  -dir string
        The directory to save the results within (required)
  -keep-trying int
        program will exit when unable to find a better match this many times in a row (default 500)
  -max-token-length int
        the maximum length of a token (required)
  -midway-target int
        aggressive until this point, beneath this the full dataset is used for every worker (default 500000)
  -overlap int
        how much overlap in the dataset given to each worker until midway (default 4)
  -strips int
        number of strips to distribute to the workers (default 100)
  -vocab int
        vocabulary size, e.g. 65535 (required)
  -workers int
        number of worker threads to run, excluding main thread (default 79)
```

Once completed the final best vocabulary will have its filename written to stdout and you'll find it in `-dir`. It's compressed, so to uncompress it you need to run it through `./exporttokens` like this:
```
./exporttokens dir/123456789_1234.zlib whatever
```
This will result in 2 files:
```
whatever.txt
whatever.bin
```
The .txt file is just a list of the tokens separated, but since newlines can be in the tokens, it's only for looking at.
The .bin file is the tokenizer.

It's very likely I will change the format of the tokenizer within the next few weeks, but if so I'll provide a way to convert the old format to the new one.
