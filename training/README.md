## Training TokenMonster

First prepare your dataset. The "extract_text_from_jsonl_parquet.py" file can help with this, as many of these datasets are in either jsonl or parquet. To use for generating a tokenizer you need raw text.
It's important that the dataset represents how you want to tokenize. You need to carefully consider what is in it. For example, if it's largely formal language, then the tokenizer will prefer formal language and might decide informal language is not worth giving tokens to. If you want a balance between English and French, then you'll need to have 50% English and 50% French in the training data. You see?

Then you'll need to compile `getalltokens.go`, `trainvocab.go` & `exporttokens.go`, preferably with Go version of at least 1.20:
```
go mod init tokenmonster
go mod tidy
go build getalltokens.go
go build trainvocab.go
go build exporttokens.go
```

First run `./getalltokens` on your training data. The default settings require around 100GB of RAM. If you don't have that you'll need to reduce `-chunk-size` to something much lower, like 10,000,000 bytes or even 1,000,000. However, you then need to consider the effect of `-min-occur-chunk`, perhaps reduce it to 2. You can probably increase `-min-occur` to something much higher without losing any quality, I kept it low just in case. Do not increase `-workers` higher than 1 unless you have more than 512 GB of RAM.
```
Usage of ./getalltokens:
  -capcode
        use alternative uppercase encoding (default false)
  -charset string
        One of: UTF-8, UTF-16, binary (required)
  -chunk-size int
        the number of bytes processed at a time, higher is faster but it means more RAM requirements (default 100000000)
  -dataset string
        filename of the dataset plain-text (required)
  -include-single-bytes
        If you enable this single byte tokens will also be recorded (default false)
  -max-token-length int
        the maximum length of a token (default 32)
  -micro-chunks int
        The higher this number, the slower it is but it will reduce peak memory usage (default 1)
  -min-occur int
        tokens will be trimmed if they occur less frequently than this in the dataset (default 100)
  -min-occur-chunk int
        tokens will be trimmed if they occur less frequently than this per chunk (default 3)
  -output string
        output filename for the dictionary (required)
  -workers int
        Multi-threading, also multiplies RAM requirements, you can't have more workers than chunks (default 1)
```

Then run `./trainvocab` passing the filename of the file created by `getalltokens` as the `-dictionary`. It'll use `-dir` directory (creating it if it does not exist) to store various states while it's working, which you can also use to resume it from that point if for any reason you stop and start it again. `-workers` defaults to the number of CPU threads you have minus 1. You can speed it up by reducing `-midway-target` to 200,000 and reducing `-percentage`, but the final vocabulary might be less optimal. The `-charset` and `-capcode` variables are not stored anywhere, so you must remember to specify these arguments exactly the same to all three executables or bad things will happen.
```
Usage of ./trainvocab:
  -capcode
        expect capcode encoding, which modifies ungreedy behavior (default false)
  -charset string
        One of: UTF-8, UTF-16, binary (required)
  -dataset string
        filename of the dataset plain-text (required)
  -dictionary string
        filename of the dictionary generated by getalltokens or any of the saved output files from this app (required)
  -dir string
        The directory to save the results within (required)
  -keep-trying int
        program will exit when unable to find a better match this many times in a row (default 1000)
  -max-token-length int
        the maximum length of a token (required)
  -midway-target int
        aggressive until this point, beneath this the full dataset is used for every worker (default 500000)
  -no-reserve-256
        disable default behavior of including 256 tokens representing every single byte (default false)
  -percentage int
        percentage of the dataset given to each worker before midway (default 15)
  -strips int
        number of strips to distribute to the workers (default 100)
  -vocab int
        vocabulary size, e.g. 65535 (required)
  -workers int
        number of worker threads to run, excluding main thread (default 79)
```
In case you want to stop `trainvocab` for any reason, there is `interval` file in the `-dir` directory, saved at intervals, which you can use as the `-dictionary` input and it'll continue from that point. You can also use one of the final vocabularies as `-dictionary` and `trainvocab` will attempt to keep-trying using that as the current best vocabulary. Any file in `-dir` can be used as the `-dictionary` to resume from that point.

If you want to be fancy, you can use different datasets for `getalltokens` and `trainvocab`. Doing so will help avoid overfitting.

When finished, the final best vocabulary will have its filename written to stdout, and it's also the first file alphabetically in `-dir`. It's compressed and contains only tokens, to convert it to a vocabulary you use `./exporttokens` like this:
```
./exporttokens dir/123456789_1234.zlib vocab_name -capcode -charset UTF-8 -txt
```
This will result in 2 files:
```
vocab_name.vocab
vocab_name.txt
```
The .txt file is just a list of the tokens separated, but it's only for looking at. It contains one token per line but tokens can contain new lines, so don't be confused if it has a different number of lines than you expect. The .vocab file is the final vocabulary file.

`-capcode` and `-charset` should be the same as the values you were using previously.
